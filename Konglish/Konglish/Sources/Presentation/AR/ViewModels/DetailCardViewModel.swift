//
//  DetailCardViewModel.swift
//  Konglish
//
//  Created by Apple MacBook on 7/24/25.
//

import Foundation
import AVFoundation
import Speech
import SwiftData

@Observable
class DetailCardViewModel: NSObject {
    // MARK: - ëª¨ë¸ ìƒíƒœ
    var word: CardModel?
    var accuracyType: AccuracyType = .btnMic
    var isBossCard: Bool = false
    var heart: Int = 5
    var currentScore: Int = 0
    var accuracyPercent: Int = 0
    
    // MARK: - ë°œìŒ ê²°ê³¼ ì €ì¥ìš©
    var lastEvaluatedScore: Int? = nil
    var lastPassed: Bool = false
    
    // MARK: - ìŒì„± ë ˆë²¨
    var voiceLevel: Float = 0.0
    
    // MARK: - ìŒì„± í•©ì„± (TTS)
    let speechSynthesizer = AVSpeechSynthesizer()
    
    var finishedCards: Set<CardModel> = []
    
    func speakWord() {
        guard let word = word?.wordEng else {
            print("ë°œìŒí•  ë‹¨ì–´ ì—†ìŒ")
            return
        }
        
        let utterance = AVSpeechUtterance(string: word)
        utterance.voice = AVSpeechSynthesisVoice(language: "en-US")
        utterance.rate = 0.5
        utterance.pitchMultiplier = 1.1
        speechSynthesizer.speak(utterance)
    }
    // MARK: - ìƒíƒœ
    var recordingState: RecordingState = .idle

    // MARK: - ì˜¤ë””ì˜¤
    var recognitionRequest: SFSpeechAudioBufferRecognitionRequest?
    var recognitionTask: SFSpeechRecognitionTask?
    let audioEngine = AVAudioEngine()
    let speechRecognizer = SFSpeechRecognizer()

    // MARK: - ë…¹ìŒ ì‹œì‘
    func startRecording() {
        cleanupAudio()

        SFSpeechRecognizer.requestAuthorization { status in
            guard status == .authorized else {
                print("ê¶Œí•œ ì—†ìŒ")
                return
            }

            DispatchQueue.main.async {
                do {
                    let session = AVAudioSession.sharedInstance()
                    try session.setCategory(.playAndRecord, mode: .default, options: [.defaultToSpeaker, .allowBluetooth])
                    try session.setActive(true, options: .notifyOthersOnDeactivation)
                } catch {
                    print("AVAudioSession ì„¤ì • ì‹¤íŒ¨: \(error)")
                    return
                }

                self.recognitionRequest = SFSpeechAudioBufferRecognitionRequest()
                self.recognitionRequest?.shouldReportPartialResults = true

                guard let request = self.recognitionRequest else { return }

                let inputNode = self.audioEngine.inputNode
                let format = inputNode.inputFormat(forBus: 0)

                inputNode.removeTap(onBus: 0)
                inputNode.installTap(onBus: 0, bufferSize: 1024, format: format) { buffer, _ in
                    request.append(buffer)
                    self.updateVoiceLevel(from: buffer)
                }

                self.recognitionTask = self.speechRecognizer?.recognitionTask(with: request) { _, _ in }

                do {
                    try self.audioEngine.start()
                    self.recordingState = .recording
                } catch {
                    print("ë…¹ìŒ ì‹œì‘ ì‹¤íŒ¨: \(error)")
                }
            }
        }
    }

    // MARK: - ë…¹ìŒ ì¤‘ë‹¨
    func stopRecording() {
        guard recordingState == .recording else {
            print("ì¤‘ë‹¨ ëª»í•´ì”€")
            return
        }
        audioEngine.stop()
        audioEngine.inputNode.removeTap(onBus: 0)
        recognitionRequest?.endAudio()
        recordingState = .readyToEvaluate
    }

    // MARK: - í‰ê°€ ì‹¤í–‰ (í´ë¡œì € ë°©ì‹)
    func evaluate() {
        guard recordingState == .readyToEvaluate else { return }

        recordingState = .evaluating

        var didFinish = false

        self.recognitionTask = self.speechRecognizer?.recognitionTask(with: self.recognitionRequest!) { result, error in
            if didFinish { return }

            if let result = result, result.isFinal {
                let spokenRaw = result.bestTranscription.formattedString
                let spoken = self.normalize(spokenRaw)
                let target = self.normalize(self.word?.wordEng ?? "")
                let score = self.calculateSimilarityScore(spoken: spoken, target: target)

                print("ğŸ¤ ì¸ì‹ ê²°ê³¼: \(spokenRaw)")
                print("ğŸ“Š ì ìˆ˜: \(Int(score * 100))")

                self.evaluatePronunciation(scorePercent: Int(score * 100))

                didFinish = true
                self.cleanupAudio()
                self.recordingState = .idle
            }

            if let error = error {
                print("ì¸ì‹ ì˜¤ë¥˜: \(error.localizedDescription)")
                didFinish = true
                self.evaluatePronunciation(scorePercent: 0)
                self.cleanupAudio()
                self.recordingState = .idle
            }
        }
    }
    
    /// ìš°ì„  ì‹œì—°ì„ ìœ„í•´ ëœë¤ ì ìˆ˜ë¥¼ ë°˜í™˜í•œë‹¤.
    func evaluateStub() { // TODO: ìŒì„± ì¸ì‹ ê³ ì¹œ í›„ ì‚­ì œ í•„ìš”
        self.cleanupAudio()
        self.recordingState = .idle
        
        self.accuracyPercent = Int.random(in: 60..<100)
        switch accuracyPercent {
        case 100:
            updateScore(base: 5)
        case 90..<100:
            updateScore(base: 4)
        case 80..<90:
            updateScore(base: 3)
        case 70..<80:
            updateScore(base: 2)
        case 60..<70:
            updateScore(base: 1)
        default:
            print("ë°œìŒ ì‹¤íŒ¨, í•˜íŠ¸ -1")
            heart = max(0, heart - 1)
            accuracyType = .failure
            lastPassed = false
            lastEvaluatedScore = nil
        }
        
        if let word = word {
            finishedCards.insert(word)
        }
    }
    
    private func updateScore(base: Int) {
        let finalScore = isBossCard ? base * 3 : base
        currentScore += finalScore
        lastEvaluatedScore = finalScore
        lastPassed = true
        accuracyType = .success
        print("ì ìˆ˜ íšë“: \(finalScore)ì  \(isBossCard ? "(ë³´ìŠ¤ Ã—3)" : "")")
    }

    // MARK: - ì •ë¦¬
    private func cleanupAudio() {
        recognitionTask?.cancel()
        recognitionTask = nil
        recognitionRequest = nil

        if audioEngine.isRunning {
            audioEngine.stop()
        }

        audioEngine.inputNode.removeTap(onBus: 0)
    }

    private func updateVoiceLevel(from buffer: AVAudioPCMBuffer) {
        guard let channelData = buffer.floatChannelData?[0] else { return }
        let array = Array(UnsafeBufferPointer(start: channelData, count: Int(buffer.frameLength)))
        let rms = sqrt(array.map { $0 * $0 }.reduce(0, +) / Float(buffer.frameLength))
        let power = 20 * log10(rms)
        let normalized = max(0, min(1, (power + 50) / 50))

        DispatchQueue.main.async {
            self.voiceLevel = normalized
        }
    }

    private func normalize(_ string: String) -> String {
        string.lowercased().trimmingCharacters(in: .whitespacesAndNewlines)
    }

    private func calculateSimilarityScore(spoken: String, target: String) -> Float {
        return spoken == target ? 1.0 : 0.0
    }
    
    private func evaluatePronunciation(scorePercent: Int) {
           print("ğŸ’¯ ìµœì¢… ì ìˆ˜: \(scorePercent)")
       }
}


enum RecordingState {
    case idle              // ë…¹ìŒ ì•ˆí•¨
    case recording         // ë…¹ìŒ ì¤‘
    case readyToEvaluate   // ë…¹ìŒ ì¤‘ë‹¨ í›„ í‰ê°€ ëŒ€ê¸°
    case evaluating        // í‰ê°€ ì¤‘
}
