//
//  DetailCardViewModel.swift
//  Konglish
//
//  Created by Apple MacBook on 7/24/25.
//

import Foundation
import AVFoundation
import Speech
import SwiftData

@Observable
class DetailCardViewModel: NSObject {
    // MARK: - ëª¨ë¸ ìƒíƒœ
    var word: CardModel?
    var accuracyType: AccuracyType = .failure
    var isBossCard: Bool = false
    var heart: Int = 3
    var currentScore: Int = 0
    
    // MARK: - ë°œìŒ ê²°ê³¼ ì €ì¥ìš©
    var lastEvaluatedScore: Int? = nil
    var lastPassed: Bool = false
    
    // MARK: - ìŒì„± ë ˆë²¨
    var voiceLevel: Float = 0.0
    
    // MARK: - ìŒì„± í•©ì„± (TTS)
    let speechSynthesizer = AVSpeechSynthesizer()
    
    func speakWord() {
        guard let word = word?.wordEng else {
            print("ë°œìŒí•  ë‹¨ì–´ ì—†ìŒ")
            return
        }
        
        let utterance = AVSpeechUtterance(string: word)
        utterance.voice = AVSpeechSynthesisVoice(language: "en-US")
        utterance.rate = 1.0
        utterance.pitchMultiplier = 1.1
        speechSynthesizer.speak(utterance)
    }
    
    // MARK: - ë°œìŒ í‰ê°€
    private let speechRecognizer = SFSpeechRecognizer(locale: Locale(identifier: "en-US"))
    private let audioEngine = AVAudioEngine()
    private var recognitionRequest: SFSpeechAudioBufferRecognitionRequest?
    private var recognitionTask: SFSpeechRecognitionTask?
    var recordingState: RecordingState = .idle
    var isRecording: Bool { recordingState == .recording }

    // ì˜¤ë””ì˜¤ ê´€ë ¨ ì†ì„±
    var recognitionRequest: SFSpeechAudioBufferRecognitionRequest?
    var recognitionTask: SFSpeechRecognitionTask?
    let audioEngine = AVAudioEngine()
    let speechRecognizer = SFSpeechRecognizer()

    // ë§ˆì´í¬ ë ˆë²¨ í‘œì‹œìš©
    var voiceLevel: Float = 0.0

    // ë‹¨ì–´
    var word: CardModel?

    // MARK: - ë²„íŠ¼ ëˆŒë €ì„ ë•Œ ë™ì‘
    func toggleRecording() {
        switch recordingState {
        case .idle:
            startRecording()
        case .recording:
            Task {
                await stopAndEvaluate()
            }
        case .evaluating:
            // í‰ê°€ ì¤‘ì´ë©´ ë¬´ì‹œ
            break
        }
    }

    // MARK: - ë…¹ìŒ ì‹œì‘
    private func startRecording() {
        cleanupAudio()

        SFSpeechRecognizer.requestAuthorization { status in
            guard status == .authorized else {
                print("Speech recognition not authorized")
                return
            }

            DispatchQueue.main.async {
                do {
                    let session = AVAudioSession.sharedInstance()
                    try session.setCategory(.record, mode: .measurement, options: .duckOthers)
                    try session.setActive(true, options: .notifyOthersOnDeactivation)
                } catch {
                    print("AVAudioSession ì„¤ì • ì‹¤íŒ¨: \(error)")
                    return
                }

                self.recognitionRequest = SFSpeechAudioBufferRecognitionRequest()
                self.recognitionRequest?.shouldReportPartialResults = true

                guard let request = self.recognitionRequest else { return }

                let inputNode = self.audioEngine.inputNode
                let format = inputNode.inputFormat(forBus: 0)

                inputNode.removeTap(onBus: 0)
                inputNode.installTap(onBus: 0, bufferSize: 1024, format: format) { buffer, _ in
                    request.append(buffer)
                    self.updateVoiceLevel(from: buffer)
                }

                self.recognitionTask = self.speechRecognizer?.recognitionTask(with: request) { _, _ in }

                do {
                    try self.audioEngine.start()
                    self.recordingState = .recording
                } catch {
                    print("audioEngine start ì‹¤íŒ¨: \(error)")
                }
            }
        }
    }

    // MARK: - ë…¹ìŒ ë©ˆì¶”ê³  í‰ê°€
    private func stopAndEvaluate() async {
        recordingState = .evaluating

        audioEngine.stop()
        audioEngine.inputNode.removeTap(onBus: 0)
        recognitionRequest?.endAudio()

        guard let task = recognitionTask else { return }

        await withCheckedContinuation { continuation in
            var didFinish = false

            recognitionTask = speechRecognizer?.recognitionTask(with: recognitionRequest!) { result, error in
                if didFinish { return }

                if let result = result, result.isFinal {
                    let spokenRaw = result.bestTranscription.formattedString
                    let spoken = self.normalize(spokenRaw)
                    let target = self.normalize(self.word?.wordEng ?? "")
                    let score = self.calculateSimilarityScore(spoken: spoken, target: target)

                    print("ğŸ¤ ì¸ì‹ ê²°ê³¼: \(spokenRaw)")
                    print("ğŸ“Š ì ìˆ˜: \(Int(score * 100))")

                    self.evaluatePronunciation(scorePercent: Int(score * 100))

                    didFinish = true
                    self.cleanupAudio()
                    self.recordingState = .idle
                    continuation.resume()
                }

                if let error = error {
                    print("ì¸ì‹ ì—ëŸ¬: \(error.localizedDescription)")
                    didFinish = true
                    self.evaluatePronunciation(scorePercent: 0)
                    self.cleanupAudio()
                    self.recordingState = .idle
                    continuation.resume()
                }
            }
        }
    }

    // MARK: - ì˜¤ë””ì˜¤ ì´ˆê¸°í™”
    private func cleanupAudio() {
        recognitionTask?.cancel()
        recognitionTask = nil
        recognitionRequest = nil

        if audioEngine.isRunning {
            audioEngine.stop()
        }

        audioEngine.inputNode.removeTap(onBus: 0)
    }

    private func updateVoiceLevel(from buffer: AVAudioPCMBuffer) {
        guard let channelData = buffer.floatChannelData?[0] else { return }
        let array = Array(UnsafeBufferPointer(start: channelData, count: Int(buffer.frameLength)))
        let rms = sqrt(array.map { $0 * $0 }.reduce(0, +) / Float(buffer.frameLength))
        let power = 20 * log10(rms)
        let normalized = max(0, min(1, (power + 50) / 50))

        DispatchQueue.main.async {
            self.voiceLevel = normalized
        }
    }

    private func normalize(_ string: String) -> String {
        string.lowercased().trimmingCharacters(in: .whitespacesAndNewlines)
    }

    private func calculateSimilarityScore(spoken: String, target: String) -> Float {
        // ê°„ë‹¨í•œ ì˜ˆ: Levenshtein ê±°ë¦¬ë¡œ ìœ ì‚¬ë„ ê³„ì‚°
        return spoken == target ? 1.0 : 0.0
    }

    private func evaluatePronunciation(scorePercent: Int) {
        // í‰ê°€ ê²°ê³¼ ì²˜ë¦¬
        print("ê²°ê³¼ ì ìˆ˜: \(scorePercent)")
    }
}


enum RecordingState {
    case idle
    case recording
    case evaluating
}
